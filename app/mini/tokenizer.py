# tokenizer.py
from __future__ import annotations

import re
from typing import List, FrozenSet

# =========================================================
# Stopwords (must match indexing pipeline)
# =========================================================

english_stopwords: FrozenSet[str] = frozenset([
    "during", "as", "whom", "no", "so", "shouldn't", "she's", "were", "needn", "then", "on",
    "should've", "once", "very", "any", "they've", "it's", "it", "be", "why", "ma", "over",
    "you'll", "they", "you've", "am", "before", "shan", "nor", "she'd", "because", "been",
    "doesn't", "than", "will", "they'd", "not", "those", "had", "this", "through", "again",
    "ours", "having", "himself", "into", "i'm", "did", "hadn", "haven", "should", "above",
    "we've", "does", "now", "m", "down", "he'd", "herself", "t", "their", "hasn't", "few",
    "and", "mightn't", "some", "do", "the", "we're", "myself", "i'd", "won", "after",
    "needn't", "wasn't", "them", "don", "further", "we'll", "hasn", "haven't", "out", "where",
    "mustn't", "won't", "at", "against", "shan't", "has", "all", "s", "being", "he'll", "he",
    "its", "that", "more", "by", "who", "i've", "o", "that'll", "there", "too", "they'll",
    "own", "aren't", "other", "an", "here", "between", "hadn't", "isn't", "below", "yourselves",
    "ve", "isn", "wouldn", "d", "we", "couldn", "ain", "his", "wouldn't", "was", "didn", "what",
    "when", "i", "i'll", "with", "her", "same", "you're", "yours", "couldn't", "for", "doing",
    "each", "aren", "which", "such", "mightn", "up", "mustn", "you", "only", "most", "of", "me",
    "she", "he's", "in", "a", "if", "but", "these", "him", "hers", "both", "my", "she'll", "re",
    "weren", "yourself", "is", "until", "weren't", "to", "are", "itself", "you'd", "themselves",
    "ourselves", "just", "wasn", "have", "don't", "ll", "how", "they're", "about", "shouldn",
    "can", "our", "we'd", "from", "it'd", "under", "while", "off", "y", "doesn", "theirs",
    "didn't", "or", "your", "it'll"
])

corpus_stopwords = [
    "category", "references", "also", "external", "links",
    "may", "first", "see", "history", "people", "one", "two",
    "part", "thumb", "including", "second", "following",
    "many", "however", "would", "became"
]

all_stopwords: FrozenSet[str] = english_stopwords.union(corpus_stopwords)

# =========================================================
# Tokenization regex (same as indexing)
# =========================================================
RE_WORD = re.compile(r"""[#@\w](['-]?\w){2,24}""", re.UNICODE)


def tokenize(text: str) -> List[str]:
    """
    Tokenize text using EXACTLY the same pipeline as indexing:
    - lowercase
    - RE_WORD tokenization
    - stopword removal
    """
    if not text:
        return []
    text = text.lower()
    out: List[str] = []
    for m in RE_WORD.finditer(text):
        tok = m.group()
        if tok and tok not in all_stopwords:
            out.append(tok)
    return out
