{"cells":[{"cell_type":"code","execution_count":2,"id":"a2d04fea","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Config OK\n"]}],"source":["# ===== FULL METADATA BUILD (NO index rebuild, NO PageRank recompute) =====\n","\n","import os\n","import pickle\n","import re\n","import builtins\n","from google.cloud import storage\n","\n","# Bucket where you already have: postings_gcp/ and pr/\n","BUCKET_NAME = \"207400714-task3\"\n","\n","# New folder to store metadata (as requested: no version subfolder)\n","META_PREFIX = \"metadata\"   # will create: gs://BUCKET/metadata/*.pkl\n","\n","# Local temp output (Dataproc/GCP notebook VM)\n","LOCAL_OUT_DIR = \"/content/full_metadata_out\"\n","os.makedirs(LOCAL_OUT_DIR, exist_ok=True)\n","\n","client = storage.Client()\n","print(\"Config OK\")\n"]},{"cell_type":"code","execution_count":3,"id":"d9be00d9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: long (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- text: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 2:======================================================>(122 + 2) / 124]\r"]},{"name":"stdout","output_type":"stream","text":["Loaded corpus rows (approx): 6348910\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Read full corpus parquet (same idea as your PageRank code uses multistream*)\n","# NOTE: This assumes your corpus lives under gs://BUCKET/multistream*\n","corpus_df = spark.read.parquet(f\"gs://{BUCKET_NAME}/multistream*\")\n","\n","# Make sure these columns exist in your corpus parquet:\n","# id, title, text\n","corpus_df.select(\"id\", \"title\", \"text\").printSchema()\n","\n","docs_rdd = corpus_df.select(\"id\", \"title\", \"text\").rdd\n","print(\"Loaded corpus rows (approx):\", corpus_df.count())\n"]},{"cell_type":"code","execution_count":4,"id":"2af50037","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizer ready\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Tokenization MUST match the indexing pipeline (Assignment 3 / your GCP notebook)\n","\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# If stopwords are not available in the environment:\n","nltk.download(\"stopwords\")\n","\n","english_stopwords = frozenset(stopwords.words(\"english\"))\n","corpus_stopwords = [\n","    \"category\", \"references\", \"also\", \"external\", \"links\",\n","    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","    \"many\", \"however\", \"would\", \"became\"\n","]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","# EXACT regex used in your GCP notebook:\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","def clean_tokens_for_dl(text: str):\n","    \"\"\"Return tokens exactly like indexing: lowercase + RE_WORD + stopword removal.\"\"\"\n","    if not text:\n","        return []\n","    tokens = []\n","    for m in RE_WORD.finditer(text.lower()):\n","        tok = m.group()\n","        if tok in all_stopwords:\n","            continue\n","        tokens.append(tok)\n","    return tokens\n","\n","print(\"Tokenizer ready\")\n"]},{"cell_type":"code","execution_count":5,"id":"7662bd13","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Saved titles.pkl and doc_len.pkl\n","N = 6348910\n"]}],"source":["def build_meta(rows_iter):\n","    \"\"\"\n","    For each document:\n","      - doc_id = int(id)\n","      - title = string (fallback to \"\")\n","      - dl = number of tokens in text after SAME tokenization as indexing\n","    \"\"\"\n","    out = []\n","    for r in rows_iter:\n","        doc_id = int(r[\"id\"])\n","        title  = r[\"title\"] if r[\"title\"] is not None else \"\"\n","        text   = r[\"text\"] if r[\"text\"] is not None else \"\"\n","        dl = len(clean_tokens_for_dl(text))\n","        out.append((doc_id, title, dl))\n","    return iter(out)\n","\n","meta = docs_rdd.mapPartitions(build_meta).collect()\n","\n","titles = {doc_id: title for doc_id, title, dl in meta}\n","doc_len = {doc_id: dl    for doc_id, title, dl in meta}\n","\n","with open(os.path.join(LOCAL_OUT_DIR, \"titles.pkl\"), \"wb\") as f:\n","    pickle.dump(titles, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open(os.path.join(LOCAL_OUT_DIR, \"doc_len.pkl\"), \"wb\") as f:\n","    pickle.dump(doc_len, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","print(\"Saved titles.pkl and doc_len.pkl\")\n","print(\"N =\", len(doc_len))\n"]},{"cell_type":"code","execution_count":6,"id":"2d2d4be3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved corpus_stats.pkl: {'N': 6348910, 'total_tokens': 2028354650, 'avgdl': 319.4807691398996, 'note': 'computed from doc_len.pkl using the SAME tokenization as indexing'}\n"]}],"source":["N = len(doc_len)\n","total_tokens = builtins.sum(doc_len.values())\n","avgdl = (total_tokens / N) if N else 0.0\n","\n","corpus_stats = {\n","    \"N\": int(N),\n","    \"total_tokens\": int(total_tokens),\n","    \"avgdl\": float(avgdl),\n","    \"note\": \"computed from doc_len.pkl using the SAME tokenization as indexing\"\n","}\n","\n","with open(os.path.join(LOCAL_OUT_DIR, \"corpus_stats.pkl\"), \"wb\") as f:\n","    pickle.dump(corpus_stats, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","print(\"Saved corpus_stats.pkl:\", corpus_stats)\n"]},{"cell_type":"code","execution_count":7,"id":"d6afba67","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["PageRank CSV columns not as expected. Falling back to header=False...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Saved pagerank.pkl entries: 6345849\n"]}],"source":["from pyspark.sql import functions as F\n","\n","# Your bucket already has pr/ (from the full PageRank you computed)\n","pr_raw = spark.read.csv(f\"gs://{BUCKET_NAME}/pr\", header=True, inferSchema=True)\n","\n","# Robust handling: if header is wrong/missing, fallback to header=False\n","cols = set(pr_raw.columns)\n","if not {\"id\", \"pagerank\"}.issubset(cols):\n","    print(\"PageRank CSV columns not as expected. Falling back to header=False...\")\n","    pr_raw = spark.read.csv(f\"gs://{BUCKET_NAME}/pr\", header=False, inferSchema=True)\n","    # assume: _c0=id, _c1=pagerank\n","    pr_df = pr_raw.select(\n","        F.col(\"_c0\").cast(\"int\").alias(\"id\"),\n","        F.col(\"_c1\").cast(\"double\").alias(\"pagerank\")\n","    )\n","else:\n","    pr_df = pr_raw.select(\n","        F.col(\"id\").cast(\"int\").alias(\"id\"),\n","        F.col(\"pagerank\").cast(\"double\").alias(\"pagerank\")\n","    )\n","\n","pr_pairs = pr_df.rdd.map(lambda r: (int(r[\"id\"]), float(r[\"pagerank\"]))).collect()\n","pagerank = dict(pr_pairs)\n","\n","with open(os.path.join(LOCAL_OUT_DIR, \"pagerank.pkl\"), \"wb\") as f:\n","    pickle.dump(pagerank, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","print(\"Saved pagerank.pkl entries:\", len(pagerank))\n"]},{"cell_type":"code","execution_count":8,"id":"99f59744","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file:///content/full_metadata_out/titles.pkl [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","| [1/1 files][168.9 MiB/168.9 MiB] 100% Done                                    \n","Operation completed over 1 objects/168.9 MiB.                                    \n","Copying file:///content/full_metadata_out/doc_len.pkl [Content-Type=application/octet-stream]...\n","/ [1/1 files][ 44.3 MiB/ 44.3 MiB] 100% Done                                    \n","Operation completed over 1 objects/44.3 MiB.                                     \n","Copying file:///content/full_metadata_out/corpus_stats.pkl [Content-Type=application/octet-stream]...\n","/ [1/1 files][  137.0 B/  137.0 B] 100% Done                                    \n","Operation completed over 1 objects/137.0 B.                                      \n","Copying file:///content/full_metadata_out/pagerank.pkl [Content-Type=application/octet-stream]...\n","- [1/1 files][ 84.7 MiB/ 84.7 MiB] 100% Done                                    \n","Operation completed over 1 objects/84.7 MiB.                                     \n","✅ Uploaded metadata to: gs://207400714-task3/metadata/\n"]}],"source":["# Upload to the bucket into: gs://BUCKET/metadata/\n","!gsutil -m cp \"{LOCAL_OUT_DIR}/titles.pkl\"       \"gs://{BUCKET_NAME}/{META_PREFIX}/titles.pkl\"\n","!gsutil -m cp \"{LOCAL_OUT_DIR}/doc_len.pkl\"      \"gs://{BUCKET_NAME}/{META_PREFIX}/doc_len.pkl\"\n","!gsutil -m cp \"{LOCAL_OUT_DIR}/corpus_stats.pkl\" \"gs://{BUCKET_NAME}/{META_PREFIX}/corpus_stats.pkl\"\n","!gsutil -m cp \"{LOCAL_OUT_DIR}/pagerank.pkl\"     \"gs://{BUCKET_NAME}/{META_PREFIX}/pagerank.pkl\"\n","\n","print(\"✅ Uploaded metadata to:\", f\"gs://{BUCKET_NAME}/{META_PREFIX}/\")\n"]},{"cell_type":"code","execution_count":10,"id":"9730d3bf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["     137 B  2026-01-06T22:12:49Z  gs://207400714-task3/metadata/corpus_stats.pkl\r\n","  44.3 MiB  2026-01-06T22:12:47Z  gs://207400714-task3/metadata/doc_len.pkl\r\n"," 84.69 MiB  2026-01-06T22:12:52Z  gs://207400714-task3/metadata/pagerank.pkl\r\n","168.88 MiB  2026-01-06T22:12:44Z  gs://207400714-task3/metadata/titles.pkl\r\n","TOTAL: 4 objects, 312329495 bytes (297.86 MiB)\r\n"]}],"source":["!gsutil ls -lh gs://207400714-task3/metadata/\n"]},{"cell_type":"code","execution_count":null,"id":"43abb922","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}